{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51a6e0a-ac67-47d8-beca-4f447aff73a1",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410c3f6-4f59-427f-bf27-6a6ec8e5c938",
   "metadata": {},
   "source": [
    "Running a model is usually the easiest part of model building.  What ends up being a bigger challenge is model evaluation, and in this module we will go through estimator selection framework, understand how to initialize and train a model, and spend most of our time in model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be9f5e-7bf4-4ff5-9a0f-17fbd775fd67",
   "metadata": {},
   "source": [
    "## Estimator Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21948183-1b4b-4595-98ec-b5e4beeaf938",
   "metadata": {},
   "source": [
    "scikit learn has a pretty helpful chart for choosing the right model type.  There's a few key decisions that you will need to make to get to the best model type:\n",
    "- are you predicting a categorical variable or a continuous one?\n",
    "    - if the variable is continous, are your variables bounded in any way (e.g. [0, 1], [-1, 1])\n",
    "- how many data samples do you have for training?\n",
    "- are there any statistical properties of your data that you need to deal with, e.g.:\n",
    "    - degree of outliers / low sample features that can cause overfitting\n",
    "    - very high feature:sample size ratio\n",
    "    - multi-collinearity\n",
    "- do you need any intermediate outputs from the model?  e.g.:\n",
    "    - do you need to get feature importance?\n",
    "    - do you need to explain the decision boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc7422-2927-48db-8896-281053c35ead",
   "metadata": {},
   "source": [
    "![Model Type](ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5b199-5c27-43cb-be61-c3650931bd57",
   "metadata": {},
   "source": [
    "## Building a model and predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7b8cf-9979-4fe6-9b64-350b02994138",
   "metadata": {},
   "source": [
    "We can build any model by:\n",
    "1. initializing the model and\n",
    "2. calling the `fit` method with our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2665ad-f9f7-40a0-bb49-cf6ecfae47e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a203f-ac9b-412d-8483-7874baa870bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f822a-6ada-4347-a783-09eae2e9157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c035fd-ed3a-4bb3-85a2-9733a381fab7",
   "metadata": {},
   "source": [
    "after we have fitted the model, we can now call `predict` to get a prediction on the target variable given inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa99c7-095a-4aea-ad92-e153f7404894",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict([[4, 4], [6, 6], [4, 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b25b5-1ef9-4aee-a74c-cfa58bc99766",
   "metadata": {},
   "source": [
    "and modeling is done!\n",
    "\n",
    "However, how do we know that this model is good?  How do we define what good is?\n",
    "\n",
    "To solve this problem, we will need to introduce the following concepts:\n",
    "1. splitting our data set into training and test data\n",
    "2. use a model evaluation metric to test the performance of our model\n",
    "3. cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab01f7-b798-4a4c-bfa5-6d772db3185c",
   "metadata": {},
   "source": [
    "## Training, Testing and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0cb47d-0db7-43a9-9e0a-2502e0f57346",
   "metadata": {},
   "source": [
    "The primary goal of building a model is to use seen samples to infer a relationship between features and the target, and use this model to predict unseen samples.  The classic example of this is to use historical prices to build a model that predicts future prices of an asset.\n",
    "\n",
    "One major issue that arises is overfitting - it is very easy to build a model that fits so well to the intricacies of the seen samples, but does not generalize well to predict unseen samples, which defeats the goal of building the model.\n",
    "\n",
    "To help mitigate this, it's best practice during model training to hold out a part of the available samples as a test set.  This way we will have a better gauge of whether the trained model can generalize to out of sample data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f47957-7df4-45ff-8540-f2d4e2b154aa",
   "metadata": {},
   "source": [
    "To do this with sklearn we can leverage `train_test_split` to split our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a8de9-8414-47a0-8264-1495cdee32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64ff91-a704-450e-9654-059c46230bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(random_state=0, n_features=1, noise=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce3758-109e-4245-a7ce-4a4e734a7bb3",
   "metadata": {},
   "source": [
    "In the example above, `train_test_split` will randomly hold out 40% of our sample data to use for testing, and return the appropriate X, y datasets from both training and testing.  We can then use the train datasets to train the model and test on the test dataset.\n",
    "\n",
    "First we can verify that test is 40% of the total data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6c3c7-98f9-4906-afb4-7547a89a5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cf88e-af92-421b-9f97-c535e00e3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119dd30-7b3e-4a86-9e8f-b2c0632ce0f8",
   "metadata": {},
   "source": [
    "Next, we can use the training data set to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedf3f9-c6cd-4807-aa74-d760af01a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811cc53-e35f-4c90-b05c-6a4e67217809",
   "metadata": {},
   "source": [
    "And finally, we can assess the quality of our model by using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44619348-2e95-4ae5-8a29-82acc884ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e48042-e4ea-4f87-b5e1-faafa45d294d",
   "metadata": {},
   "source": [
    "Now we have a single metric to show how our trained model performs against an out of sample test set.\n",
    "\n",
    "We can also use other types of metrics dpeending on our problem space.  For example:\n",
    "- mean absolute error may be more appropriate than mean squared error (e.g. if large outliers are not something you want to penalie the model for):\n",
    "- explained variance can be good for measuring how much variability you're explaining with the model\n",
    "- max error captures the _worst_ error that can be generated\n",
    "We simply need to import the metrics, and evaluatel them using the model priction and the test target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918047be-b77e-49fd-b755-30bd107ae7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, max_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005eb2d-4634-4e65-9804-50bcb9fc1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a92fcf-6aa2-421a-98e3-4b707cdde742",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccecef-3645-42cd-8f19-84541658e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b838178-e698-4246-a371-e8e81dea0778",
   "metadata": {},
   "source": [
    "Using these metrics we can now engineer and transform features and change the model type to try to lower the target metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237590e-960e-4498-aade-3d2131ef4c96",
   "metadata": {},
   "source": [
    "In addition, depending on the problem space, the type of emtric we need to use can be very different also.  For example, if we are performing a classification, we probably want to look at `roc_curve` or `precision_recall_curve`, whereas if we're predicting a continuous variable we're more likely to use `mean_squared_error`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff82f23-0e48-4247-8a25-e30d4f2d77a2",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f4d14-d95d-47e4-96c1-adac5b0403e3",
   "metadata": {},
   "source": [
    "One note on time series - because the data for time series is time dependent, using `train_test_split` will not work because it will randomly take a set of samples to be test samples.  This is a problem for time series since the data is not randomly ordered - we should not be able to see data points in the future when we train our model otherwise it will have futuresight.  \n",
    "\n",
    "Luckily, sklearn has `TimeSeriesSplit` which allows us to easily split out train and test data sets while making sure that we do not accidentally see into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c7260-1478-41c9-8acd-ec7b7ce15797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a178816-aa14-45a4-b5b0-abae504a64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cfb68-189b-4642-a977-578f491a45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(n_splits=3, test_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945deaae-ef3c-4923-afb4-783ebeb5280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in tss.split(X):\n",
    "    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6a871-d914-4d1c-9cd5-320a45aaf59e",
   "metadata": {},
   "source": [
    "we can now iteratively predict on the next [n] items, using all historical data afor training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15444899-6fa7-492b-97ff-dcd2130e78ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Validation (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d43996-4202-4b67-900f-02229f71ec48",
   "metadata": {},
   "source": [
    "In our simple example above, we used train/test splitting to help assess our model performance on out of sample data.  However, this may still not be sufficient, especially if we have models that have hyperparameters that need to be tuned.  \n",
    "\n",
    "In this situation we will need to re-run train/test multiple times with different hyperparameters to optimize them, however this can now lead to hyperparameter overfitting because we can overfit on the test data.\n",
    "\n",
    "The solution for this is to utilize a technique called cross-validation.  In this scenario, we will split the training data into k even sets (e.g. 5 sets).  Then, k-1 sets are used to train the model, and the last set will be used to evaluate the model.  This is done across all k combinations, and the performance of the model is the average of all k runs.  After the best hyperparameter is chosen, we can then evaluate it on the test set to get the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ee0cf-1214-4be0-8e02-771c9fad4040",
   "metadata": {},
   "source": [
    "![Cross Validation](cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c5d66-569f-40d2-8dcd-ba652828214d",
   "metadata": {},
   "source": [
    "An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3239c6b-48b8-42e4-9b89-a72169792b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818b3dc-0617-4b02-a6ff-5a7418ab1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb4b0f-1766-43d7-b8af-6d5fe51609fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7a2dd-e375-43b5-928c-9633abf30d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632535dd-0e94-4c77-a639-d3ceda621da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e385743-c173-4df7-821f-8d851bf01f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=0.1, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38ff76-1fb3-4f2b-8fad-026a50b387a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=100, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefa5af-b41c-435f-93f9-15ee7548ad57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

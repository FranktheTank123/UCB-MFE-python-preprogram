{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71859cc6-62cb-4c1b-b704-548432a2be0a",
   "metadata": {},
   "source": [
    "# Machine Learning & `sklearn` Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1a357-88e6-4e7a-8f01-683d87dc916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defc8c8-e1fd-4ab1-ada0-91ba2cd1384b",
   "metadata": {},
   "source": [
    "Scikit-learn (aka sklearn) is the standard library for machine learning in Python.  Like Pandas, under the hood it uses numpy, and it comes with a very powerful yet simple interface that covers 95% of the day to day model R&D and productionization work that you will need to do.\n",
    "\n",
    "In this module we'll be going over sklearn's basics and understand the process of building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb444bf9-4afd-471a-887f-0b298c3adf71",
   "metadata": {},
   "source": [
    "## Machine Learning workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2f357-d504-41bc-819c-8e2f16bfa2e9",
   "metadata": {},
   "source": [
    "At a high level, building a model is fairly simple, in that there are five main steps:\n",
    "1. ingest, and clean your data\n",
    "2. transform / feature engineer\n",
    "3. select model type and train the model(s)\n",
    "4. predict on test data, and evaluate performance\n",
    "5. iterate based on performance metrics\n",
    "\n",
    "We've already seen step 1 in the data analysis module, and we will be going through the rest of the steps in the next few modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a25efe-d268-43bb-ab5e-080451c8d6b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `sklearn` Design Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138986d5-192f-4e9b-b53a-3a74b1c374a3",
   "metadata": {},
   "source": [
    "sklearn has three fundamental interfaces:\n",
    "- the Estimator (a thing that builds a model)\n",
    "- the Predictor (a thing that can predict an output using inputs)\n",
    "- the Transformer (a thing that can take one or more rows of data, and augment the data in some way)\n",
    "\n",
    "Since these are interfaces, a specific class can be one or more of these things.  For example, a class can be an estimator (it will train using training data) and a transformer (it will augment input data) at the same time.\n",
    "\n",
    "In addition, we can compose estimators together using `pipelines` to build complex data processing and modeling tasks easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c74bc-a141-4b84-a0a2-9e47fc83d164",
   "metadata": {},
   "source": [
    "### Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437d444-845f-4a17-ab7d-80242388de78",
   "metadata": {},
   "source": [
    "Estimators in sklearn are any class that implements the `fit` method.  The `fit` method is the learning part of machine learning, where we will input training features and target variables, and allow the specific algorithm to fit a model based on the data and its hyperparameters.\n",
    "\n",
    "Let's take a look at a super simple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de4244-e416-450f-ba87-4bab9fccc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa5a7f-a9d3-49ac-8bfe-d5b0f2322802",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57467561-a52a-4662-b733-93779ac56c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression(fit_intercept=True)\n",
    "fitted = regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40498f0c-359b-4099-b5d7-72f962f828f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e5576-6054-40e1-8828-dd013b9cc31c",
   "metadata": {},
   "source": [
    "In the above example we did the following:\n",
    "1. imported the LinearRegression model\n",
    "2. created some test data\n",
    "3. Instatiated the regression\n",
    "4. called the `fit` method which trains our linear regression model using our features and targets, creating a fitted model\n",
    "\n",
    "We can verify that the linear regression ran properly as our fitted coefficients are the same as the coefficients we used to generate the data.\n",
    "\n",
    "Also, we can see that instantiating the Estimator and training the model are done in separate steps.  This means that you can instantiate an Estimator and use that estimator to train against multiple data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f4c86-7368-4720-bdfc-0db8d582246f",
   "metadata": {},
   "source": [
    "### Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72b5bf-f262-4201-927b-3bfec8fbf4cb",
   "metadata": {},
   "source": [
    "The main goal of training a model is using the fitted model to make predictions for test features.  In scikit-learn, any class that has a `predict(X, ...)` method is considered a predictor.  \n",
    "\n",
    "We actually created a predictor in the example above - the fitted model is a predictor with the `predict` method, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0f077-8ad3-42e3-a360-c184f9a2713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.predict(np.array([[3, 5], [4, 6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ed86c-6352-4f62-b270-3714753b1203",
   "metadata": {},
   "source": [
    "We can see above that after fitting the model, we're now able to predict new feature samples that are passed to the model.  \n",
    "\n",
    "**note**: the fitted model is still an estimator, in that we can all `.fit` on it again and fit new data if we wanted to, i.e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59717d4-d5df-4d39-8a9d-87abee1d2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted2 = fitted.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfafbfce-cb43-489c-b6c7-212173e51053",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981b815-ee8e-4dc4-b2ff-ccb4e831bd6e",
   "metadata": {},
   "source": [
    "Transformers are used to augment the input data in some way, and output the transformed data.  This could be for:\n",
    "- preprocessing\n",
    "- feature selection\n",
    "- dimensionality reduction\n",
    "etc.\n",
    "\n",
    "Most of the time, transformers are estimators as well.  For example, we can use the StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f71dd-a43f-41be-9e4a-babc8cf30bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02c6ca-c774-45f5-9db2-a735cec6f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be5285-8bfc-4197-91d0-f43ffae5438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528b682-47a5-4713-80ca-0d1d8eac86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e559173-e079-41a5-90f4-84a017de4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f786f4-409b-4853-9525-2b3d27fff8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e3b1a-404a-4e30-95e7-bf87e28527ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1335485-b28b-4813-af06-97d75126c8b2",
   "metadata": {},
   "source": [
    "The above defines a class that inherits from the `BaseEstimator` and the `TransformerMixin`, which means that it needs to define the `fit` method, but also now has the `fit_transform` method from the `TransformerMixin`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826dd858-3cdc-479f-adf5-b8c3c67ef8cd",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c489eb8-b618-47b4-b4eb-2866e35e074f",
   "metadata": {},
   "source": [
    "Because there are consistent interfaces across all Estimators, Transformers and Predictors, we can compose (i.e. string together) estimators to group together data processing operations and model fitting / prediction operations together.  \n",
    "\n",
    "sklearn allows us to do this easily by providing a `Pipeline` that is literally a list of Estimator objects, specifically a series of Transformers (i.e. objects that have both `fit` and `transform` methods) and a final model Estimator that just has the `fit` method.  The pipeline itself also has its own `fit` and `predict` methods, as a Pipeline is also an Estimator and a Predictor itself.\n",
    "\n",
    "Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c57a9-fa12-4d0a-a294-9e3576ab1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80039f-2e76-4c9b-b80f-ae8d1aa32b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3edcd-06d1-4a4d-8cb3-c29ac1a3b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390304f0-0527-4dd0-aa97-d1755cfab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('svc', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf05c9-2370-46ae-8bcd-9ab3f505b9d5",
   "metadata": {},
   "source": [
    "**note**: for Pipelines, you must provide a name for each step along with the estimator object, and the names must be unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3c815-cdbc-40b4-9b05-73f1bbb92d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754b9f8-cddc-4eee-a3af-ac01f592030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a781aa-35c0-4e8d-aca6-133569a358c3",
   "metadata": {},
   "source": [
    "In the above example, we have:\n",
    "1. created a Pipeline with 2 Estimator objects - a StandardScaler and an SVC model (SVM Classifier), \n",
    "2. fitted the pipeline against our training data and\n",
    "3. made a prediction with the fitted Pipeline.\n",
    "\n",
    "We can also access the individual Estimators inside the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06110621-6c15-46f7-a0ca-15d7002a87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e6c13-0384-4836-aab6-019bd24244b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59416ec-e408-4581-bbbc-52d60c1d0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['svc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ef160-ca90-46f3-9ee0-709c168dd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d27e4-ea90-4606-948b-175e703e1c2c",
   "metadata": {},
   "source": [
    "once we have the individual estimators, we can also get any coeffients we'd like from them, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817057e-27c2-48ae-b6d1-9acfbfe46bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['scaler'].mean_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

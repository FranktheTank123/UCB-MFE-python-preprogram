{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bd665d-b862-4c9d-8483-3d784a00d15a",
   "metadata": {},
   "source": [
    "Now we have seen how to build a model, let's use these concepts to build a predictive model on our data.\n",
    "\n",
    "Specifically, we are going to try to predict the return of SOL in the next hour with a simple model that uses hourly volatility and close returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002dc48-e1b7-4530-af33-d6abf2f192d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'sqlite:///../../data/data.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021ed37-e234-4bd9-a3b4-86f204c2356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253037c0-8199-406c-ad9d-2ae51f47e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_ohlc(df, lookback=10):\n",
    "    o = df.open\n",
    "    h = df.high\n",
    "    l = df.low\n",
    "    c = df.close\n",
    "    \n",
    "    k = 0.34 / (1.34 + (lookback+1)/(lookback-1))\n",
    "    cc = np.log(c/c.shift(1))\n",
    "    ho = np.log(h/o)\n",
    "    lo = np.log(l/o)\n",
    "    co = np.log(c/o)\n",
    "    oc = np.log(o/c.shift(1))\n",
    "    oc_sq = oc**2\n",
    "    cc_sq = cc**2\n",
    "    rs = ho*(ho-co)+lo*(lo-co)\n",
    "    close_vol = cc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    open_vol = oc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    window_rs = rs.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    result = (open_vol + k * close_vol + (1-k) * window_rs).apply(np.sqrt) * np.sqrt(252)\n",
    "    result[:lookback-1] = np.nan\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592bf8c-594f-4972-9fa3-06faf1bf72eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    scoring=None\n",
    "):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a199938-bb88-4b87-9fe3-5b1b4c1b814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc = pd.read_sql('SELECT * FROM ohlc', data_location)\n",
    "ohlc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a0640-0719-4092-a180-7b148b3f32a5",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ec305-4dc9-40b1-bb3e-bd7c96dda6b5",
   "metadata": {},
   "source": [
    "First, let's format the data.  right now the data set is long, i.e. each row is unique on ts/token, however we want to make each row unique on ts only.  We are converting a long table into a fat table of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98cd869-8fbe-408b-9089-7e640cdd1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ohlc.token.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d16a4-9d55-4117-9dcb-f50547e73cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_merge(left, right):\n",
    "    return pd.merge(left, right, on='ts', how='inner')\n",
    "\n",
    "X = reduce(df_merge, [\n",
    "    (lambda df: \n",
    "    (\n",
    "        df\n",
    "        .assign(\n",
    "            vol=vol_ohlc(df).fillna(0),\n",
    "            ret=df.close.pct_change()\n",
    "        )[['ts', 'vol', 'ret']]\n",
    "        .rename(columns={\n",
    "            col: f'{col}_{token}' for col in ['vol', 'ret'] if col != 'ts'\n",
    "        })\n",
    "    ))(ohlc[ohlc.token == token])\n",
    "    for token in tokens\n",
    "]).set_index('ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7543f97-8793-4c8a-b99a-5078801423af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07657695-1cbb-483d-ae45-2ac95d09c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9280b0f-3c34-4ee9-9f86-88394d75271e",
   "metadata": {},
   "source": [
    "let's separate out our X and y variables now.  We want to make sure that we are shifting the y variable by 1 step, since we want to predict 1 step into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ac4e6-2419-4aaf-b2ba-16df8ad626a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.ret_SOL.shift(-1)[:-1]\n",
    "X = X[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094ccd3-a3e4-4f37-922a-59c62ce31337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c6318-fefb-4e95-bc0f-af5c7ca7717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7879777-3457-4c93-bae0-04d955c35461",
   "metadata": {},
   "source": [
    "Let's analyze our data a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8931c9-3a91-4e62-9bd7-d7a7da5be1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ad7de-32c0-4e9f-92d6-5f2708aae848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix, autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef0d8c-134c-4e3f-9b5d-0b9f8513c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(y[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bdc214-4eb9-4586-b074-d06d32eae146",
   "metadata": {},
   "source": [
    "we can see that the hourly returns for aave doesn't have any strong autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11cdcd-fa6d-4971-9e0d-ef3f305d1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151987b-b2ea-418d-8aa5-3f06d4d7f9a0",
   "metadata": {},
   "source": [
    "we see that we have some nulls, so our transformers should deal with this.  The reason we have these is because the first value in our series doesn't have a return, so we should impute these to be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62dd52e-7f13-4d18-a54a-c12fcc4c75f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "{col: y.corr(X[col]) for col in X.columns if X[col].dtype != 'object'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b7056-e60c-4c16-ad75-8bc6a2413d97",
   "metadata": {},
   "source": [
    "finally, we can take a quick look at the correlation between y and our X variables.  We see here that the correlations are pretty weak on a column by column basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa600e-7b36-448e-811a-c0aa50b1ed11",
   "metadata": {},
   "source": [
    "Next, let's set up a set of transformers so that we can augment our data before it hits the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b69ab-5c44-4053-9a1b-b656c2cb57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eae381-861f-4520-ad8d-4a761cfec9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd7300-9a0a-4464-a27a-f025ecf480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, test_size=0.2):\n",
    "    cv = TimeSeriesSplit(n_splits=int(y.shape[0] * test_size), test_size=1)\n",
    "    scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "    \n",
    "    return np.mean(cross_validate(model, X, y, cv=cv, scoring=scorer, n_jobs=-1)['test_score'])\n",
    "    \n",
    "    # scores = []\n",
    "    # for train_idx, test_idx in cv.split(X):\n",
    "    #     X_train, X_test, y_train, y_test = X.iloc[train_idx], X.iloc[test_idx], y.iloc[train_idx], y.iloc[test_idx]\n",
    "    #     pipeline.fit(X_train, y_train)\n",
    "    #     score = mean_squared_error(y_test, pipeline.predict(X_test), squared=False)\n",
    "    #     scores.append(score)\n",
    "        \n",
    "    # return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077ef44-c402-4d50-81b6-486ffd8e479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('feature_selector', FeatureSelector(['ret_AAVE'])),\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', DecisionTreeRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757431f-0c95-46ed-a293-899241bec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', DecisionTreeRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacdd2b-0620-4e14-8c44-2afc680106a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('model', DecisionTreeRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b5a5c-df62-4ce7-aaac-8247b8f55822",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('feature_selector', FeatureSelector(['ret_AAVE'])),\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=0))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028802b-ef72-4b4c-8e1a-c02dae3e8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    # ('pca', PCA(n_components=5)),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=0))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5ce7c-d0f5-41ec-a364-5d4c8415c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=0))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb83ab-7f07-4388-b342-ba12d70ad64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('model', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e96731-8977-4ef5-9d36-2ebb2c775c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('pca', PCA(n_components=1)),\n",
    "    ('model', Ridge(alpha=1.))\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aca704-c297-418a-baa2-2cd446776488",
   "metadata": {},
   "source": [
    "The above looks pretty good, however let's optimize the alpha parameter on the Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ec625-6e6e-42a9-85f1-050119db45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "test_size = 0.2\n",
    "cv = TimeSeriesSplit(n_splits=int(y.shape[0] * test_size), test_size=1)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "\n",
    "search = GridSearchCV(pipeline, {\n",
    "    'pca__n_components': [1, 5, 10, 20],\n",
    "    'model__alpha': [0.1, 0.5,  1.]\n",
    "}, scoring=scorer, refit=True, cv=cv, n_jobs=-1)\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ced98-94dc-4f60-b202-232643d71bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b159b7-eeb1-407e-8a3e-4d6b9079d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811f063-0064-47c5-88ca-c932021cc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0de43f-5164-4fe7-a039-5f4bfa83fdcb",
   "metadata": {},
   "source": [
    "Let's check the learning curve of our best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17696e23-e9fa-4e95-825c-379c1b660ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "title = \"Learning curves for ridge regression\"\n",
    "\n",
    "plot_learning_curve(\n",
    "    best_model, title, X, y, axes=axes, cv=cv, n_jobs=4, scoring=scorer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117e29a-aff8-407a-a597-51c58ddce2da",
   "metadata": {},
   "source": [
    "Now that we have completed the model, we will need to save it down so that we can load it up in our model.  To do this, we will use pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b242a00-2afd-4270-8925-e77264bf108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model, open('best_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bc542-6e49-48af-a970-c4827344a59f",
   "metadata": {},
   "source": [
    "We can verify that this is the same model that we saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7906deb-e521-481b-a87e-06512af8673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('best_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace17a9b-e8e9-442b-817b-59913b1aab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict(X.iloc[[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632017e0-fcab-485d-995f-04bb865e0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(X.iloc[[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4f5c6-5831-4ee4-8c11-b82393580760",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b207541-6a62-43fb-a4d4-e1ddb75f07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95a97e-b232-4b7d-b621-3e4047e7b819",
   "metadata": {},
   "source": [
    "we can now take this model and build a server around it, so that other systems can also now make predictions on the hourly returns of SOL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864d8779-c905-4fe3-859b-cb8680e24422",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291611c1-9e2b-4652-a2ef-01979300bb70",
   "metadata": {},
   "source": [
    "In the previous class, we used train/test splitting to help assess our model performance on out of sample data.  However, this may still not be sufficient, especially if we have models that have hyperparameters that need to be tuned.  \n",
    "\n",
    "In this situation we will need to re-run train/test multiple times with different hyperparameters to optimize them, however this can now lead to hyperparameter overfitting because we can overfit on the test data.\n",
    "\n",
    "The solution for this is to utilize a technique called cross-validation.  In this scenario, we will split the training data into k even sets (e.g. 5 sets).  Then, k-1 sets are used to train the model, and the last set will be used to evaluate the model.  This is done across all k combinations, and the performance of the model is the average of all k runs.  After the best hyperparameter is chosen, we can then evaluate it on the test set to get the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce85cb-4c3f-41de-8399-6ae3f257abe9",
   "metadata": {},
   "source": [
    "![Cross Validation](cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09743590-0db8-47e9-8f47-16fe0c64a4c9",
   "metadata": {},
   "source": [
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e26c63-9a34-43ea-8ae7-35218cf7e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79faf918-9ae8-4065-829e-24512a84a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b92c49-d762-417f-8e0b-930e531cbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7bded-6770-4b6f-a1a9-95079ce8cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105ef29-8539-41d5-b06b-331c7cfe66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=100, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd5092-b0ce-4924-87ae-bc8bedd2700b",
   "metadata": {},
   "source": [
    "In the above examples, we pass a model, as well as the input X and y data sets into the `cross_val_score` method.  The method will generate `cv` number of folds, and then train on the training set, predict on the fold's test set and evaluate the goodness of the model based on its `score` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358d587-85c5-454c-8af3-f1d4a82d2b0f",
   "metadata": {},
   "source": [
    "While works nicely if there's a `score` method in the model, we might want to evaluate the model in some customized manner.  In that situation we will need to manually generate the folds for cross validation, then apply logic on the training and test data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127f3d3-f484-4edd-b14c-217cd0cec0b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating Cross-validation indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2adae71-be3e-4922-ae64-5be06679f4d4",
   "metadata": {},
   "source": [
    "To run kfolds manually, we can use the `KFolds` class to instantiate a KFold iterator, then split our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924c0ad-17e9-49d5-a47e-73bdb761b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727444b-e217-4558-a5e9-93e18ce1cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485a92f-a55a-4fa3-9060-be6f946aa6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in cv.split(range(20)):\n",
    "    print(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eadec3-85c1-4f87-85db-f450b1d62787",
   "metadata": {},
   "source": [
    "We can see from the above that kfolds chunks the data set into 5 separate chunks and in turn creates training and test indices for each iteration.  With these indices we can select the appropriate rows within the original X and y data sets to get our training and test sets.\n",
    "\n",
    "There's also other strategies to cross validate, for example, we can run a leave one out strategy, where all samples except one is used for training, and only 1 sample is used for test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d1e6e-4789-4476-b541-56bd1ec2cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6036883-44b2-416e-9c6d-29535622851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = LeaveOneOut()\n",
    "for train_idx, test_idx in cv.split(range(20)):\n",
    "    print(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c433453-03bc-4ee9-8302-d3b0df0b039d",
   "metadata": {},
   "source": [
    "We can also shuffle and split the data, where we can specify the number of validations we want to do, as well as how large the test size is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7198e69-6ecb-451c-8bea-0beda44fb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa255e74-abb8-45e4-bbfe-b468b62e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "for train_idx, test_idx in cv.split(range(20)):\n",
    "    print(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdbd54-1be1-4062-829c-113db61c1d5b",
   "metadata": {},
   "source": [
    "so far, all the cross-validation strategies are focused on the feature set only.  However, we may want to create validation sets that are more balanced against the target variable - in this case there is a class of \"stratified\" cross validation strategies, which tries to create the same percentage of each target class in the complete set.  This is very useful for classification problems, where you don't want super skewed tests in some validations.\n",
    "\n",
    "For example, the stratified version of KFold is Stratified KFold, where the main difference is that we need to put in the target variable to help with the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31cf59-26f4-46b1-8590-718dac1874f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aedc27-f267-4de5-8d5e-ecd06959d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c5633-4d29-41a9-8f05-7f4dc926fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=3)\n",
    "for train, test in cv.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(np.bincount(y[train]), np.bincount(y[test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb51fdb-11c5-48de-8d77-f2860909615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3)\n",
    "for train, test in cv.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(np.bincount(y[train]), np.bincount(y[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee7859-a512-4dd2-9640-810639795a00",
   "metadata": {},
   "source": [
    "In addition, there are stratified versions of the non-stratified cross validation strategies (e.g. Stratified Shuffle Split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24921a3b-ff59-48e2-9b60-652f86bb0e23",
   "metadata": {},
   "source": [
    "Lastly, as mentioned before, for time series specifically, we can use TimeSeriesSplit which will sequentially add more data to the training set, while using \"unobserved\" data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b0c26-6ffa-4068-a018-a65829540f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89181b-1fde-4f90-9726-e9e8972ef78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9b4f7-f1ed-42c5-b3da-418fc808b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in cv.split(range(20)):\n",
    "    print(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5d47a-0aa4-4e8e-9023-ce64575bddc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5cc54-aa0c-4e0a-a81f-71f5ed75396b",
   "metadata": {},
   "source": [
    "Now that we know how to create cross validation sets, we can tune hyperparameters of our model.  The general way to do this is to use a search technique to search across the space of hyperparameters given the data.  \n",
    "\n",
    "Firstly, we can figure out what our available parameters are via the `get_params` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88214f-29a4-49af-9189-a18e48e74fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a99a7-24d6-447e-babf-954508ef2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654c2e8-cc93-42b4-9421-24d451005836",
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2b7f9-aae4-47ff-abc5-a68246cec5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002ca19-40b8-41a2-b03e-ff426fc7f5f7",
   "metadata": {},
   "source": [
    "The simplest way to search for hyper-parameters is using a grid search, which will search across all dimensions specified in parameters.  To do this, we can wrap our base model into a grid search, and calling fit will run the grid search and identify the best model based on the model's `score` method, or some custom `scoring` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb6673-be12-4bc3-aa00-7ff5c3f23dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc55cd-b4b3-4f0a-b597-0916d12f6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047b213-49ca-4d6b-adec-52f05bada754",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883a19c-420b-40b9-956e-c3ffda8b17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters, scoring=None)\n",
    "\n",
    "clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec3b2d-d7e3-4db6-8b64-9c8ad2e3c679",
   "metadata": {},
   "source": [
    "after we have searched, we can now get our best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d8962-f2b2-4c37-980f-ae0d9cd8ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa230f5-ced2-49e2-a22f-0e2e6581c2a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extra: Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8a56f-feee-4b55-831f-1f4e5c3ccfdd",
   "metadata": {},
   "source": [
    "Once we can do basic cross validation, we can get a lot of very interesting data using the technique.  One example is that we can plot a learning curve, which shows the score of an Estimator for varying numbers of training samples.  This allows us to understand how much additional data will impact our model both in terms of its ability to generalize as well as its runtime.\n",
    "\n",
    "In the example below, we can see that the SVM is much better for learning our digits data, however the cost of execution increases very aggressively with more samples, in a more than linear fashion.  This means that this model will get into trouble during training if the data set is large enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392decd1-6678-41bf-95d2-1f3e186f300b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47454598-52b7-4c6f-b5e8-3ad5b01c1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67735f4-e7ca-49e8-9859-1172f9b578ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "# Cross validation with 50 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = GaussianNB()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "estimator = SVC(gamma=0.001)\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843bc47-6bf5-4c67-8309-5e970447a4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94945d9d-2720-4658-b834-73ea688b1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146dfd1-355f-4238-9dfc-3a7d5ff1e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('hw3.csv')\n",
    "raw_data.info()\n",
    "raw_data['token'].value_counts()\n",
    "#raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a929a-bbc0-4503-9a8c-34d8283ce6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = raw_data.duplicated().sum()\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a42b88-a116-4c7a-acfb-35f10b5eeffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.drop_duplicates()\n",
    "raw_data.loc[raw_data['token'].str.contains('<span '), 'token'] = raw_data.loc[raw_data['token'].str.contains('<span '), 'token'].str.extract('<span>(.|\\n)*?</span>')\n",
    "raw_data['ts'] = pd.to_datetime(raw_data['ts'])\n",
    "raw_data.info()\n",
    "raw_data['token'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e98c7-4f40-45f8-abd0-e5807143565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['chain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec840df-272b-4443-b46d-593fe756758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = raw_data.loc[(raw_data.close / raw_data.open >= 2.0) | (raw_data.close / raw_data.open <= 0.5) | (raw_data['close'].isnull())]\n",
    "x\n",
    "# y = raw_data.loc[(raw_data.close.isnull() & (raw_data.high.isnull() | raw_data.low.isnull()))] \n",
    "# There are a total of 33 rows where the close value is null and the high or low value is null.\n",
    "# One of these rows has null values for all three columns. We can try sorting by token and ts and use ffill()\n",
    "# to obtain an estimated close price. However as only 33/2360 (~1.4%) rows have this issue, I instead chose to drop these rows.\n",
    "# y\n",
    "# len(y)\n",
    "# 100 * (len(y)/2360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d719a-037b-4c6a-9379-d361db1a0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that we have 2360 non-null/duplicate rows in this data set\n",
    "# some rows must have nulls in the following columns: token, high, low, and close\n",
    "# close has some null values and some bad values (total 169 rows), \n",
    "# what complicates our cleaning efforts is that some null close rows have missing high or low values\n",
    "# I have elected to drop these null rows as I couldn't think of a reliable heuristic to generate close values for these rows\n",
    "raw_data['close'] = raw_data['close'].mask((raw_data['close'].isnull() | \n",
    "                                            (raw_data.close/raw_data.open >= 2.0)\n",
    "                                           | (raw_data.close/raw_data.open <= 0.5)), (0.5 * (raw_data['high'] + raw_data['low'])))\n",
    "raw_data.dropna(subset=['close'], inplace=True)\n",
    "raw_data = raw_data.reset_index()\n",
    "raw_data.info()\n",
    "# If I run the previous cell again after running this one, x will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b67d76-780e-4e11-b357-246e0858c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb19996-48b5-48e9-aeae-8c773f8e0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have refactored the code in the following cell from the data cleaning example notebook in lecture 3\n",
    "tokens = raw_data.token.unique()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=math.ceil(tokens.size / 2), ncols=2, figsize=(15, 5 * math.ceil(tokens.size / 2)))\n",
    "\n",
    "idx = 0\n",
    "for label, t in raw_data[['token', 'close']].groupby('token'):\n",
    "    t['close'].plot(ax=axes[idx // 2, idx % 2], label=label)\n",
    "    axes[idx // 2, idx % 2].legend()\n",
    "    \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa095b8-793b-4c27-8a2a-c7dc9ab30bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "chains = raw_data.chain.unique()\n",
    "fig, axes = plt.subplots(nrows=math.ceil(chains.size / 2), ncols=2, figsize=(15, 5 * math.ceil(chains.size / 2)))\n",
    "idx = 0\n",
    "for label, ch in raw_data[['chain', 'close']].groupby('chain'):\n",
    "    ch['close'].plot(ax=axes[idx // 2, idx % 2], label=label)\n",
    "    axes[idx // 2, idx % 2].legend()\n",
    "    \n",
    "    idx += 1\n",
    "'''\n",
    "# Plotting close prices by token is emminently more relevant since in the chain case \n",
    "#   the 'ETH plot' will contain close prices for multiple tokens and is not as useful. \n",
    "#   On the otherhand the chain column contains no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca96fa-6ad3-4f25-8620-bdfd50e981fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed a spike in close prices for USDT. I wanted to ensure \n",
    "#    that this was just a spike that naturally occurred in the market.\n",
    "usdt = raw_data.loc[raw_data['chain'] == 'USDT'].sort_values(by=['close'], ascending=False)\n",
    "#usdt.head(25)\n",
    "usdt\n",
    "#Indeed this spike seems to have naturally occurred in the market between 12/4-12/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efc5c0-0382-4eb3-ba6a-80854f154e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining step before calculating the volumeUSD by chain is\n",
    "#    to examine the volume column and look for/cleanup any irregularities.\n",
    "tokens = tokens[0:-1]\n",
    "for t in tokens:\n",
    "    current_token = raw_data.loc[raw_data['token'] == str(t)]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    fig.suptitle(str(t)+' Volume')\n",
    "    axes[0].hist(current_token['volume'])\n",
    "    axes[0].set_xlabel('Volume')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[1].plot(current_token['volume'])\n",
    "    axes[1].set_ylabel('Volume')\n",
    "    fig\n",
    "    print(current_token['volume'].describe())\n",
    "# For each token, min and max are not too far from 25th and 75th percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1abc74-245f-46dd-af2d-cec6b2e05b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a handful of volume outliers for each token, given the volatility of cryptocurrencies,\n",
    "#   none of the spikes are particularly abnormal. Even the most extreme outliers are pretty much within 5*mean.\n",
    "# From these three approaches, there are likely no anomalies within the volume data.\n",
    "# We are ready to proceed with calculating the volumeUSD by chain.\n",
    "raw_data['volumeUSD'] = raw_data['close'] * raw_data['volume']\n",
    "volume_USD = raw_data.groupby('chain')['volumeUSD'].sum().to_frame()\n",
    "volume_USD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bd665d-b862-4c9d-8483-3d784a00d15a",
   "metadata": {},
   "source": [
    "Now we have seen how to build a model, let's use these concepts to build a predictive model on our data.\n",
    "\n",
    "Specifically, we are going to try to predict the return of SOL in the next hour with a simple model that uses hourly volatility and close returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "60f4098c-ebf7-4b07-91db-e56791162238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00856016244338903"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location = 'sqlite:///../../../data/data.db'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pickle\n",
    "\n",
    "def vol_ohlc(df, lookback=10):\n",
    "    o = df.open\n",
    "    h = df.high\n",
    "    l = df.low\n",
    "    c = df.close\n",
    "    \n",
    "    k = 0.34 / (1.34 + (lookback+1)/(lookback-1))\n",
    "    cc = np.log(c/c.shift(1))\n",
    "    ho = np.log(h/o)\n",
    "    lo = np.log(l/o)\n",
    "    co = np.log(c/o)\n",
    "    oc = np.log(o/c.shift(1))\n",
    "    oc_sq = oc**2\n",
    "    cc_sq = cc**2\n",
    "    rs = ho*(ho-co)+lo*(lo-co)\n",
    "    close_vol = cc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    open_vol = oc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    window_rs = rs.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    result = (open_vol + k * close_vol + (1-k) * window_rs).apply(np.sqrt) * np.sqrt(252)\n",
    "    result[:lookback-1] = np.nan\n",
    "    \n",
    "    return result\n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    scoring=None\n",
    "):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "        scoring=scoring,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "ohlc = pd.read_sql('SELECT * FROM ohlc', data_location)\n",
    "## Data formatting\n",
    "\n",
    "\n",
    "tokens = ohlc.token.unique()\n",
    "\n",
    "def df_merge(left, right):\n",
    "    return pd.merge(left, right, on='ts', how='inner')\n",
    "\n",
    "X = reduce(df_merge, [\n",
    "    (lambda df: \n",
    "    (\n",
    "        df\n",
    "        .assign(\n",
    "            vol=vol_ohlc(df).fillna(0),\n",
    "            ret=df.close.pct_change()\n",
    "        )[['ts', 'vol', 'ret']]\n",
    "        .rename(columns={\n",
    "            col: f'{col}_{token}' for col in ['ts', 'vol', 'ret'] if col != 'ts'\n",
    "        })\n",
    "    ))(ohlc[ohlc.token == token])\n",
    "    for token in tokens\n",
    "]).set_index('ts')\n",
    "\n",
    "y = X.ret_SOL.shift(-1)[:-1]\n",
    "X = X[:-1]\n",
    "\n",
    "# my model here\n",
    "from itertools import combinations\n",
    "\n",
    "# feature engineering: \n",
    "# demean returns window 5:\n",
    "ret = X.loc[:, [c for c in X.columns if 'ret' in c]]\n",
    "vol = X.loc[:, [c for c in X.columns if 'vol' in c]]\n",
    "# demean_ret = pd.concat([X.loc[:,'ret_'+str(t)].apply for t in tokens])\n",
    "demean_ret = ret.apply(lambda x : x - x.rolling(5).mean())\n",
    "demean_ret.columns = ['demean_'+x for x in ret.columns]\n",
    "X = pd.concat([X, demean_ret], axis = 1)\n",
    "\n",
    "\n",
    "# clustering: \n",
    "def get_cluster(ls):\n",
    "    comb_list = list(combinations(ls, 2))\n",
    "    dist_list = [np.abs(comb[0]-comb[1]) for comb in comb_list]\n",
    "    return np.mean(dist_list)\n",
    "\n",
    "X.loc[:,'ret_cluster'] = ret.apply(get_cluster, axis = 1)\n",
    "X.loc[:,'vol_cluster'] = ret.apply(get_cluster, axis = 1)\n",
    "\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.)),\n",
    "    ('scale', StandardScaler()),\n",
    "    # ('pca', PCA(n_components=10)),\n",
    "    ('pca', PCA()),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "evaluate_model(pipeline, X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "cv = TimeSeriesSplit(n_splits=int(y.shape[0] * test_size), test_size=1)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "\n",
    "search = GridSearchCV(pipeline, {\n",
    "    'pca__n_components': [1, 5, 10, 20, 22],\n",
    "    'model__alpha': [0.1, 0.5,  1.]\n",
    "}, scoring=scorer, refit=True, cv=cv, n_jobs=-1)\n",
    "search.fit(X, y)\n",
    "\n",
    "# print(X.head(10))\n",
    "\n",
    "search.best_params_\n",
    "best_model = search.best_estimator_\n",
    "evaluate_model(best_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5925764c-a57e-4799-9c38-d676df47d856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('impute', SimpleImputer(fill_value=0.0, strategy='constant')),\n",
       "  ('scale', StandardScaler()),\n",
       "  ('pca', PCA(n_components=22)),\n",
       "  ('model', Ridge(alpha=0.1))],\n",
       " 'verbose': False,\n",
       " 'impute': SimpleImputer(fill_value=0.0, strategy='constant'),\n",
       " 'scale': StandardScaler(),\n",
       " 'pca': PCA(n_components=22),\n",
       " 'model': Ridge(alpha=0.1),\n",
       " 'impute__add_indicator': False,\n",
       " 'impute__copy': True,\n",
       " 'impute__fill_value': 0.0,\n",
       " 'impute__missing_values': nan,\n",
       " 'impute__strategy': 'constant',\n",
       " 'impute__verbose': 0,\n",
       " 'scale__copy': True,\n",
       " 'scale__with_mean': True,\n",
       " 'scale__with_std': True,\n",
       " 'pca__copy': True,\n",
       " 'pca__iterated_power': 'auto',\n",
       " 'pca__n_components': 22,\n",
       " 'pca__random_state': None,\n",
       " 'pca__svd_solver': 'auto',\n",
       " 'pca__tol': 0.0,\n",
       " 'pca__whiten': False,\n",
       " 'model__alpha': 0.1,\n",
       " 'model__copy_X': True,\n",
       " 'model__fit_intercept': True,\n",
       " 'model__max_iter': None,\n",
       " 'model__normalize': 'deprecated',\n",
       " 'model__positive': False,\n",
       " 'model__random_state': None,\n",
       " 'model__solver': 'auto',\n",
       " 'model__tol': 0.001}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95a97e-b232-4b7d-b621-3e4047e7b819",
   "metadata": {},
   "source": [
    "we can now take this model and build a server around it, so that other systems can also now make predictions on the hourly returns of SOL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
